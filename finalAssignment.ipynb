{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcNWnBpXegy9"
      },
      "source": [
        "<h1>Final Assignment</h1>\n",
        "<h2>Task: Text Generation</h2>\n",
        "<h2>Submitted by: Mainuddin Alam Irteja</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnjWStkoJ86L"
      },
      "outputs": [],
      "source": [
        "# Installing necessary libraries\n",
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0MmbrRFnFeu"
      },
      "outputs": [],
      "source": [
        "# Loading FLAN-T5 model\n",
        "\n",
        "# Importing necessary modules\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Assigning the model name and loading the tokenizer and model\n",
        "modelName = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(modelName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aphif3TWGsLn"
      },
      "outputs": [],
      "source": [
        "# Transfer the model so that the gpu is being used\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Print out which device we're using (GPU or CPU)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYiR0KEO9AoK"
      },
      "outputs": [],
      "source": [
        "# Load the WikiText-103 dataset\n",
        "from datasets import load_dataset\n",
        "wiki_Dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n",
        "\n",
        "# Split the dataset so that it could be used for training and testing\n",
        "# test_size = 0.2 which signifies 80 percent of data for training and 20 percent for testing\n",
        "split_Dataset = wiki_Dataset.train_test_split(test_size=0.2)\n",
        "train_Dataset = split_Dataset['train']\n",
        "test_Dataset = split_Dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SEEydayKzb8U"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the dataset\n",
        "\n",
        "\"\"\"\n",
        "Function to preprocess the dataset\n",
        "\n",
        "@param givenData The dataset given to be preprocessed\n",
        "@reuturns model_inputs The preprocessed model inputs\n",
        "\"\"\"\n",
        "def preprocessDataset(givenData):\n",
        "  # Extract the raw text from the data\n",
        "  inputs = [text for text in givenData['text']]\n",
        "\n",
        "  # Tokenize the inputs for text generation\n",
        "  model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Shift the tokens by one position to create the input labels\n",
        "  # Shifting helps model predicting next token\n",
        "  labels = model_inputs['input_ids'].copy()\n",
        "\n",
        "  # Adjust labels to ignore padding tokens (-100 is used so the loss function ignores padding tokens)\n",
        "  labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "  # Attach the shifted labels to the model inputs\n",
        "  model_inputs[\"labels\"] = labels\n",
        "\n",
        "  # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "  # Return the preprocessed model inputs\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMZIhTu86SkP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
