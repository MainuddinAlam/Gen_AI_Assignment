{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1jtZtccTLk"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MainuddinAlam/Gen_AI_Project/blob/main/finalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcNWnBpXegy9"
      },
      "source": [
        "<h1>Final Assignment</h1>\n",
        "<h2>Task: Text Summarization</h2>\n",
        "<h2>Submitted by: Mainuddin Alam Irteja</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnjWStkoJ86L",
        "outputId": "7c12639c-f40a-4ec5-88a2-7cd589cce425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Installing necessary libraries\n",
        "!pip install transformers datasets torch huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0MmbrRFnFeu"
      },
      "outputs": [],
      "source": [
        "# Loading FLAN-T5 model\n",
        "\n",
        "# Importing necessary modules\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Assigning the model name and loading the tokenizer and model\n",
        "modelName = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(modelName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aphif3TWGsLn"
      },
      "outputs": [],
      "source": [
        "# Transfer the model so that the gpu is being used\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Print out which device we're using (GPU or CPU)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYiR0KEO9AoK"
      },
      "outputs": [],
      "source": [
        "# Loading CNN/DailyMail dataset\n",
        "from datasets import load_dataset\n",
        "cnn_Dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "\n",
        "# Split the dataset so that it could be used for training and evaluating\n",
        "split_Dataset = cnn_Dataset.train_test_split(test_size=0.12)\n",
        "train_Dataset = split_Dataset['train'].train_test_split(test_size=0.98)['train']\n",
        "eval_Dataset = split_Dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEEydayKzb8U"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the dataset\n",
        "\n",
        "\"\"\"\n",
        "Function to preprocess the dataset\n",
        "\n",
        "@param givenData The dataset given to be preprocessed\n",
        "@reuturns model_inputs The preprocessed model inputs\n",
        "\"\"\"\n",
        "def preprocessDataset(givenData):\n",
        "  # Extract the raw text from the data\n",
        "  inputs = [doc for doc in givenData['article']]\n",
        "\n",
        "  # Tokenize the articles with padding and truncation to a max length of 512\n",
        "  model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Tokenize the summaries\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(givenData['highlights'], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Attach the tokenized summaries as labels to the model inputs\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "  # Return the preprocessed model inputs\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMZIhTu86SkP"
      },
      "outputs": [],
      "source": [
        "# Tokenize the training and testing datasets\n",
        "tokenized_train_dataset = train_Dataset.map(preprocessDataset, batched=True)\n",
        "tokenized_eval_dataset = eval_Dataset.map(preprocessDataset, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHk1DEz8mmQ-"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Setting training parameters for text summarization\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',               # Directory to save model checkpoints\n",
        "    evaluation_strategy=\"epoch\",          # Evaluate the model at the end of each epoch\n",
        "    learning_rate=2e-5,                   # Learning rate\n",
        "    per_device_train_batch_size=8,        # Batch size for training\n",
        "    per_device_eval_batch_size=8,         # Batch size for evaluation\n",
        "    weight_decay=0.01,                    # Regularization to prevent overfitting\n",
        "    save_total_limit=3,                   # Only keep the last 3 checkpoints\n",
        "    num_train_epochs=3,                   # Number of epochs to train the model\n",
        "    predict_with_generate=True,           # Enable text generation during evaluation\n",
        "    logging_dir=\"./logs\"                  # Directory for storing training logs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWCT_Ewen3Yp"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Initializing the trainer object for text summarization\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                            # The model to be trained\n",
        "    args=training_args,                     # The training arguments adapted for text generation\n",
        "    train_dataset=tokenized_train_dataset,  # Tokenized training dataset\n",
        "    eval_dataset=tokenized_eval_dataset,    # Tokenized evaluation dataset\n",
        "    tokenizer=tokenizer                     # The tokenizer to handle input and output\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G46itggTuHx5"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nv2xQXUuMp3"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NRTZ6eQGlFX"
      },
      "outputs": [],
      "source": [
        "# Function to summarize texts\n",
        "\n",
        "\"\"\"\n",
        "Function to summarize texts\n",
        "\n",
        "@param text The text provided to the function\n",
        "@returns The summarized text\n",
        "\"\"\"\n",
        "def summarizeTexts(text):\n",
        "  # Tokenize the input text and move it to the correct device\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "  # Generate the summary using the fine-tuned model\n",
        "  summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Return the decoded summary back into text\n",
        "  return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07mx5PhVGruo"
      },
      "outputs": [],
      "source": [
        "print(summarizeTexts(\n",
        "    \"\"\"\n",
        "Soccer consists of teams playing against each other with each team having 11 players. Want me to continue?\n",
        "The playtime of soccer is 90 minutes. It is divided into two halves with each consisting of 45 minutes.\n",
        "Each time will try to score goals against the other. The one with the more goals after 90 minutes is the winner\"\n",
        "If both teams score equal goals, it is a draw.\n",
        "\"\"\"\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UefWcgIHk4eG"
      },
      "outputs": [],
      "source": [
        "# Sample demo text to test model\n",
        "demoText =  \"\"\"\n",
        "Person A: Hey, did you hear about the new project management software our company is planning to implement?\n",
        "\n",
        "Person B: Yeah, I heard a bit about it. What’s the deal with it?\n",
        "\n",
        "Person A: It’s called \"TaskFlow.\" The management thinks it’s going to streamline our workflow, especially with remote teams. It’s supposed to integrate all the tools we use, like Slack, Trello, and Google Drive, into one platform.\n",
        "\n",
        "Person B: That sounds interesting. But I’m a bit concerned about the learning curve. Is it user-friendly?\n",
        "\n",
        "Person A: From what I’ve seen, it looks pretty intuitive. They’re also planning to run a couple of training sessions to get everyone up to speed. The first one is next Monday.\n",
        "\n",
        "Person B: Okay, that helps. I guess I’ll have to attend that session. How does it compare to what we’re using now?\n",
        "\n",
        "Person A: It’s supposed to be much more efficient. We’ll be able to track project progress more easily and get real-time updates. Plus, it has built-in analytics to help us with performance tracking.\n",
        "\n",
        "Person B: That sounds promising. I just hope it doesn’t come with too many bugs at launch.\n",
        "\n",
        "Person A: Yeah, that’s always a concern with new software. But they’ve been testing it for a while now, so fingers crossed it goes smoothly.\n",
        "\n",
        "Person B: Let’s hope for the best. Thanks for the info!\n",
        "\n",
        "Person A: No problem. See you at the training!\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXj55ELZandt"
      },
      "outputs": [],
      "source": [
        "# Code for deploying the model on Hugging Face\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the repositroy name and hugging face token\n",
        "repo_Name = \"FINAL_PROJECT\"\n",
        "hf_Token = userdata.get(\"GEN_AI\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(repo_Name)\n",
        "tokenizer.save_pretrained(repo_Name)\n",
        "\n",
        "# Upload the model and tokenizer to Hugging Face\n",
        "model.push_to_hub(repo_Name, token=hf_Token)\n",
        "tokenizer.push_to_hub(repo_Name, token=hf_Token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9DguJ8vauVP"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model from hugging face for text summarization\n",
        "getModel = pipeline(\"summarization\", model=\"Mainuddin01/FINAL_PROJECT\", device=device)\n",
        "\n",
        "# Summarize the text and display it\n",
        "prompt = \"Summarize the given text: \" + demoText\n",
        "summary = model(prompt)\n",
        "print(\"Summary:\", summary[0]['summary_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}